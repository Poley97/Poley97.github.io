I"<blockquote>
  <p>论文链接 ： https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf</p>
</blockquote>

<p><img src="/assets/img/20210531/MOCOF1.png" alt="" /></p>

<h2 id="constrastive-learning-as-dictionary-look-up">Constrastive Learning as Dictionary Look-up</h2>

<p>Contrastive loss function，设计的目的是当一个query 有唯一对应的key时，两者尽可能相似时，损失小，反之，则损失大（即和其他keys相似则视为负样本）。一种常用的是 <strong>InfoNCE</strong> 如下</p>

\[\begin{equation}
\mathcal{L}_{q}=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^{K} \exp \left(q \cdot k_{i} / \tau\right)}
\end{equation}\]

<p>Contrastive loss mechanisms的几种形式如下</p>

<p><img src="/assets/img/20210531/MOCOF2.png" alt="" /></p>

<h2 id="momentum-contrast">Momentum Contrast</h2>

<p>对比学习是一种对高维连续输入建立离散字典的方法,比如图像输入。这个字典是动态的，其中的keys是随机采样的，并且keys encoder 参与学习。我们的假设是一个好的feature可以被从一个包含丰富负样本的大字典中学习到，同时这个字典keys的encoder是尽可能的保持一致性（consistent），除了evolution。基于这个冬季，提出了<strong>Momentum Contrast as described</strong>。</p>

<h3 id="dictionary-as-a-queue">Dictionary as a queue</h3>

<p>本文提出的方法的核心是以<strong>一个data samples队列来维持字典</strong>，这使得我们可以重复利用不久前的Mini-batch中编码的keys，同时还可以将字典大小与mini-batch size解耦，变为一个超参数，可以人为控制，可以远大于典型的Mini-batch size。</p>
:ET
I":,<blockquote>
  <p>论文地址
参考地址</p>
</blockquote>

<p>个人感想：这是一篇非常全面的综述，完整仔细阅读一遍基本相当于整理了一遍我之前所看过的论文，值得一读。</p>

<h1 id="introduction">Introduction</h1>

<p>目前随着3D传感器技术的快速发展，激光雷达LiDAR已经呗广泛使用，每个LiDAR都可以产生一副点云，包含了周围环境的几何信息。但是激光雷达的limitations和复杂性：</p>

<ol>
  <li>环境的多样性</li>
  <li>物体遮挡和阶段</li>
  <li>不同类别间和size区别得到的不相似表示，以及同一种类别物体，距离不同得到的不同结果</li>
  <li>表现可靠性</li>
</ol>

<p>其余自动驾驶中常用的传感器也有各自的优势和劣势</p>
<ol>
  <li>Camera-based：可以提供高密度的像素信息，可以捕捉到形状和纹理信息。但是单目相机缺少深度信息，TOF和stereo cameras可以提供深度信息，但是要付出昂贵的计算成本。stereo cameras的深度信息的误差随着距离的上升指数上升，TOF的容易受到光照的影响。</li>
  <li>RADAR，相比LiDAR具有更低的分辨率和更小的视野。它的特殊性使得它更适合其他任务，比如速度判定，短距离目标检测或者自动停车。</li>
</ol>

<p><img src="/assets/img/20210526/PCReviewT1.png" alt="" /></p>

<p>点云处理的难点：</p>

<ol>
  <li>点云数据的特性，高维、系数、非结构化的数据；</li>
  <li>高性能要求，自动驾驶需要从点云中提取特征并进行检测和分类，在实时的条件下。典型的场景是10Hz；</li>
  <li>安装的限制，安装在车上的处理模块是资源有限的。</li>
</ol>

<h2 id="点云处理的pipeline-和分类">点云处理的Pipeline 和分类</h2>

<p>主要的流程分为三个模块：</p>
<ol>
  <li><strong>Data Representation</strong>：Voxels,Frustums，Pillars，2Dprojection or raw point cloud</li>
  <li><strong>Feature Extraction</strong>: 见图</li>
  <li><strong>Detection Network modules</strong>：见图</li>
  <li><strong>Predictions Refinement Network</strong>:见图</li>
</ol>

<p><img src="/assets/img/20210526/PCReviewF1.png" alt="" /></p>

<p><img src="/assets/img/20210526/PCReviewF2.png" alt="" /></p>

<p>本文的目录结构也按上述的分类展开。</p>

<h1 id="data-representation-approaches">Data representation approaches</h1>

<p>点云是非结构化的数据，为了应用基于CNN的目标检测方法，点云自然需要一个结构化的表示来应用卷积操作。</p>

<h2 id="point-based">Point-based</h2>

<p>直接对点云处理来得到稀疏的表示。典型就是<strong>PointNet</strong>，后续被拓展到提取局部和全局特征，并启发了其他的工作，比如<strong>Voxel Feature Extractor</strong>(VoxelNet提出)，并被用于一系列文献，such as IPO, STD, PointRCNN, PointRGCN, LaserNe, PointFusion, RoarNe and PointPaiting, resort to this scheme of point cloud representation。</p>

<h2 id="voxel-based">Voxel-based</h2>

<p>通过体素化减小点云的维度，节省内存，帮助特征提取网络更高效的利用一组点提取计算局部和全局（底层和高层）特征，而不是逐点计算。</p>

<h2 id="frustum-based">Frustum-based</h2>

<p>将点云切割到一个Frustum里，比如Frustum PointNet,Frustum ConvNet and SIFRNet。</p>

<h2 id="pillar-based">Pillar-based</h2>

<p>相当于简化的体素化，忽略了z轴，比如PointPillars。</p>

<h2 id="projection-based">Projection-based</h2>

<p>将3D投影到2D，来减小高维的计算量。有多种投影方式，比如<strong>front biew(FV), range view(RV), bird’s eye vies(BEV)</strong>。他们分别相当于沿着z轴（前），x轴（右）和y轴（上）进行压缩。</p>

<p>其中<strong>BEV应该是最适合自动驾驶的</strong>，因为感兴趣目标都在同一个地面上，具有比较小的方差，所以这也是使用的<strong>最广泛的方法</strong>。相比FV具有一些优点，比如解决了遮挡问题，并且可以保持物体本身的物理尺寸，具有较小的方差，这都是FV所不具备的能力。</p>

<h1 id="data-feature-extraction-methods">Data feature extraction methods</h1>

<p>特征提取可以分为三个阶段</p>
<ol>
  <li>Local features，也就是low-level features，具有丰富的局部信息</li>
  <li>Global features，也就是high-level features，编码了点及其邻域点的几何特征</li>
  <li>Contextual feature, 在提取的最后阶段被提取出来，被期望具有丰富的定位和语义信息，并被提供给模型的final tasks。</li>
</ol>

<h2 id="point-wise">point-wise</h2>

<p>PointNet。为了提供permutation invariance（排列不变形）,使用了对称函数来提取和转换特征，比如maxpooling 和 avgpooling，缺点在于不能捕捉局部结构信息（邻域点之间）。
<img src="/assets/img/20210526/PCReviewF3.png" alt="" />
Pointnet++，为了克服PointNet的缺点而提出，通过<strong>Set Abstraction layer</strong>解决。同样也被用于calssification, region proposal 和 segmentation。</p>
<h2 id="segment-wise">segment-wise</h2>

<p>先将点云分割成若干个空间尺度的scenes，然后再提取特征，典型的就是<strong>Voxel Feature Extractor</strong>。和点特征提取不同，这种特征往往直接用于volumetric representation of the point cloud，比如voxels,pillars or frustums。使用这种特征提取的算法有VoxelNe, Second, Voxel-FPN, and HVNet.</p>

<p>虽然segment-wise和point-wise关联性很强，<strong>但是segment-wise的特征效率更高，更鲁邦（因为用了一组点而不是一个），允许提取更复杂的3D局部形状信息</strong>。</p>

<p>但是为了解决点云在体素间分布不均匀的问题，算法中对体素中的点随机采样，并限制了体素中点的个数。这样就会导致点多的体素丢弃一些点，<strong>造成信息损失，是的网络预测的结果可能不稳定</strong>，点少的体素又需要0填充，<strong>增加了计算量和计算资源的需求</strong>。体素的大小也需要平衡，<strong>大的体素增加推断速度，而小的可以提供更丰富的信息</strong>，因此，VEFs也被应用到提取单size voxel特征以及多size voxel中。</p>

<p><img src="/assets/img/20210526/PCReviewF4.png" alt="" /></p>
<h2 id="object-wise">Object-wise</h2>
<p>利用2D检测器来过滤点云。通过2D BBox 检测得到物体，然后转换为一个3D BBox，减小了空间感兴趣区域的搜索范围和需要处理的点数量。常见于LiDAR与其他传感器的结合方法中。</p>
<h2 id="convolution-neural-networks">Convolution neural networks</h2>

<p>其中有些方法是用于处理2D检测的，然后被转移到处理点云上，有些是单独为3D空间设计的。</p>

<h3 id="2d-backbone">2D backbone</h3>

<p>一些传统的，常用的，比如ResNet，DetNet,Hourglass Network等等。</p>

<h3 id="3d-backbone">3D backbone</h3>

<p>直接使用3D卷积，但是这并不划算，因为复杂度高，而且体素特征一般是稀疏的，对于这一点应该加以利用。</p>

<h3 id="sparse-convolutional-networks">Sparse convolutional networks</h3>

<p>当输入点有activate的点时，才计算该点的卷积。但是这样会逐层减小数据稀疏的程度，因为一个activate的点总是会使得多个输出activate，造成dilation。这使得其在深度网络的应用效果一般。为了解决这个问题，提出了Vaild Sparse Convolution ，也就是Submanifold Sparse Convolution。其对比如下图所示
，其只计算activate点对应输出位置的点的响应。</p>

<p><img src="/assets/img/20210526/PCReviewF5.png" alt="" /></p>

<h3 id="cnn-networks-based-on-voting-scheme">CNN networks based on voting scheme</h3>

<p>另一种稀疏卷积的方法，不详细介绍，同样会导致dilation。</p>

<p><img src="/assets/img/20210526/PCReviewF6.png" alt="" /></p>

<h3 id="graph-convolution-network">Graph convolution network</h3>

<p>图神经网络可以引入一些特征，比如</p>
<ol>
  <li>residual skip connections</li>
  <li>dynamic receptive fields</li>
  <li>dilatation</li>
</ol>

<p>但是由于GCN的高计算和内存负担，但自动驾驶又需要实时性，因此GCNs只用在网络流程的第四阶段，也就是refine中，因为一般此时输入的点数远少于原始输入，此时只包含proposal出来的点。</p>

<p>用于自动驾驶的GCNs算法包括 <strong>EdgeConV,MRGCN</strong>,其余还有<strong>PointRGCN</strong>等。</p>

<h2 id="feature-extraction-paradigms-in-3d-object-detection-models">Feature extraction paradigms in 3D object detection models</h2>

<p>这部分和通用目标检测没啥区别，主要将几种常见的提取特征的网络结构。</p>

<p><img src="/assets/img/20210526/PCReviewF7.png" alt="" /></p>

<p>上述若干结构的问题在于会损失信息，由于不断降采样，导致位置信息误差的增大。受到FPN的启发，出现了很多top-down结构的特征提取网络可以弥补这一点。底层的位置信息可以直接通过lateral connection连接到top-down maps上。</p>

<p><img src="/assets/img/20210526/PCReviewF8.png" alt="" /></p>

<h1 id="detection-and-prediction-refinement-networks">Detection and prediction refinement networks</h1>

<p>检测是上述总流程的第三步。可以有很多分类方法，比如结构、物体在点云中的定位表示方法、产生rpoposal的方法、Mutil-scale feature、是否refine等等。如下图所示
<img src="/assets/img/20210526/PCReviewF9.png" alt="" /></p>

<h2 id="detector-network-architecture">Detector network architecture</h2>

<p>从2D目标检测出发，检测器可以分为<em>dual-stage detectors and single-stage detector</em>。这没啥好说的。其结构大致如下
<img src="/assets/img/20210526/PCReviewF10.png" alt="" /></p>
<h2 id="detector-settings">Detector settings</h2>

<p>使用长方体或者segmentation-masks来做检测。</p>

<p>第一种就是传统的使用预定义anchors等方向来直接回归。</p>

<p>第二种是先预测一个mask，之后再通过pixel-wise mask来做Objects segmented。但是在视觉任务中一班选择用mask来回归其附近的BBox。</p>

<p>检测编码一遍对BBox-level的定位做编码，需要输入特征图，并且依赖BBox的标注信息来训练网络的回归能力。</p>

<p>Mask level 的同样需要mask的标注，在3D数据集中，这可以直接通过3D BBox获得（因为目标没有重叠）。为了达到检测目的，往往还需要加一个satge在分割之后做回归得到BBox。</p>

<p>本文总结了多个经典网络的特征提取方法，这个个人认为还是很具有参考性的。</p>

<p><img src="/assets/img/20210526/PCReviewT2.png" alt="" /></p>

<h2 id="detector-module-techniques">Detector module techniques</h2>

<h3 id="region-proposal-based-frameworks">Region proposal-based frameworks</h3>

<p>首先介绍了经典的几个R-CNN，这部分略了。传统的Faster R-CNN使用2k个proposal，而18年的AVOD用了80k个proposal，这体现了3D巨大的搜多空间对模型运行时间的负面影响。</p>
:ET
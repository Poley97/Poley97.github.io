I"*
<blockquote>
  <p>论文链接 ：https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_AugFPN_Improving_Multi-Scale_Feature_Learning_for_Object_Detection_CVPR_2020_paper.pdf</p>
</blockquote>

<h1 id="introduction">Introduction</h1>

<p>FPN可以分为一下三个阶段：</p>

<ol>
  <li>Before feature fusion</li>
  <li>top-down feature fusion</li>
  <li>after feature fusion</li>
</ol>

<p>每个阶段都存在一些内生的缺陷，如下：</p>

<ol>
  <li><strong>Semantic gaps between features at different levels :</strong> 在特征融合之前，特征都经过一个单独的$1 \times 1$卷积来减少channels，但是这些特征之间的semantic gaps没有被考虑。</li>
  <li><strong>Information loss of the highest-level feature map :</strong> top-down结构中最高层的feature反而会由于channels的减少而丢失信息。</li>
  <li><strong>Heuristical assignment strategy of RoIs :</strong> 普通提取RoI的时候，根据Anchor的大小从特定的层中提取，但其它层可能也会包含有用的信息。PANet从所有层中提取信息，然后经过max得到特征，但是max同样会造成信息损失，而如果不做max，又会导致维度太高，全连接层参数过多。</li>
</ol>

<p>综上，本文提出<strong>AugFPN</strong>,包含三部分：</p>
<ol>
  <li><strong>Consistent Supervision</strong></li>
  <li><strong>Ratio-invariant adaptive pooling</strong></li>
  <li><strong>Soft RoI Selection</strong></li>
</ol>

<p><img src="/assets/img/20210607/AUGFPNF1.png" alt="" /></p>

<p><img src="/assets/img/20210607/AUGFPNF2.png" alt="" /></p>

<h1 id="methodology">Methodology</h1>

<h2 id="consistent-supervision">Consistent Supervision</h2>

<p>FPN在Top-down结构中直接使用<strong>上采样+求和</strong>来融合特征，<strong>但是不同scale的特征包含了不同抽象等级（abstract levels）的特征</strong>，直接融合他们存在很大的<strong>semantic gaps</strong>，导致结果可能不是最优的特征金字塔。</p>

<p>因此提出Consistent Supervision，具体方法就是在top-down 结构上加一个RPN，对于所有的ROI做一个<strong>auxiliary loss</strong>，在不同levels的feature map上提取相同的RoI,然后通过一个回归头和分类头来得到他们各自的回归和分类结果作为损失，这就要求每一个level在包含的信息上具有一定的一致性。同时，每层使用的FC是<strong>shared</strong>的，这可以进一步强制不同层的feature map学习到类似的语义信息。</p>

<p>Loss就又auxiliary和正常的loss得到。</p>
:ET